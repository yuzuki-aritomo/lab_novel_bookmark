{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#local 読み込み\n",
    "dataPath = './datasets/'\n",
    "titlePath = './data/titledata/'\n",
    "keyPath = './data/keyworddata/'\n",
    "storyPath = './data/storydata/'\n",
    "allPath =  './data/all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer  # sumyのTokenizerと名前が被るため\n",
    "from janome.tokenfilter import POSKeepFilter, ExtractAttributeFilter\n",
    "\n",
    "text = \"\"\"転職 Advent Calendar 2016 - Qiitaの14日目となります。 少しポエムも含みます。\n",
    "今年11月にSIerからWebサービスの会社へ転職しました。\n",
    "早くから退職することを報告していたこともあって、幸いにも有給消化として１ヶ月のお休みをいただくことができました（これでも10日ほど余らせてしまいました）。\n",
    "# ・・・ (省略) ・・・\n",
    "だからこそ、有給消化期間はなんとしてでももぎ取るようにしましょう。\"\"\"\n",
    "\n",
    "# 1行1文となっているため、改行コードで分離\n",
    "sentences = [t for t in text.split('\\n')]\n",
    "for i in range(2):\n",
    "    print(sentences[i])\n",
    "# 転職 Advent Calendar 2016 - Qiitaの14日目となります。 少しポエムも含みます。\n",
    "# 今年11月にSIerからWebサービスの会社へ転職しました。\n",
    "\n",
    "# 形態素解析器を作る\n",
    "analyzer = Analyzer(\n",
    "    [UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r'[(\\)「」、。]', ' ')],  # ()「」、。は全てスペースに置き換える\n",
    "    JanomeTokenizer(),\n",
    "    [POSKeepFilter(['名詞', '形容詞', '副詞', '動詞']), ExtractAttributeFilter('base_form')]  # 名詞・形容詞・副詞・動詞の原型のみ\n",
    ")\n",
    "\n",
    "# 抽出された単語をスペースで連結\n",
    "# 末尾の'。'は、この後使うtinysegmenterで文として分離させるため。\n",
    "corpus = [' '.join(analyzer.analyze(s)) + '。' for s in sentences]\n",
    "for i in range(2):\n",
    "    print(corpus[i])\n",
    "# 転職 Advent Calendar 2016 - Qiita 14 日 目 なる 少し ポエム 含む。\n",
    "# 今年 11 月 SIer Web サービス 会社 転職 する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "# MeCab による単語への分割関数,(名詞,形容詞,動詞)のみ残す\n",
    "def MorphologicalAnalysis(sentences):\n",
    "  if sentences is np.nan:#欠損値は欠損値のまま返す\n",
    "    return np.NaN\n",
    "  result = []\n",
    "  text_list = sentences.split(\"。\")\n",
    "  for text in text_list:\n",
    "    tagger = MeCab.Tagger()\n",
    "    words = []\n",
    "    for c in tagger.parse(text).splitlines()[:-1]:\n",
    "      surface, feature = c.split('\\t')\n",
    "      pos = feature.split(',')[0]\n",
    "      if pos in ['名詞', '動詞', '形容詞', '副詞']:\n",
    "        words.append(surface)\n",
    "    result.append(\" \".join(words)+\"。\")\n",
    "  return ' '.join(result)\n",
    "\n",
    "#形態素解析結果を'mecab'に代入\n",
    "data = pd.read_csv(storyPath+'cleaning.csv', index_col=0)\n",
    "data['summary_mecab'] = data['cleaning'].apply(MorphologicalAnalysis)\n",
    "data.to_csv(storyPath+'summary_mecab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Label' from 'pandas._typing' (C:\\Users\\ngnls\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-40359cfc1958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstoryPath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'cleaning.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'summay_origin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAnalysisBySudachi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstoryPath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'summay_origin.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m             \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcacher\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m             \u001b[1;31m# we are trying to reference a dead referent, hence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[1;31m# a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1076\u001b[0m             \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquotechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m             \u001b[0mdate_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m             \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m             \u001b[0mescapechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwriters\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlibwriters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m from pandas._typing import (\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mCompressionOptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mFilePathOrBuffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Label' from 'pandas._typing' (C:\\Users\\ngnls\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_typing.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#セットアップ\n",
    "tqdm.pandas()\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "\n",
    "# sudachi による単語への分割関数,(名詞,形容詞,動詞)のみ残す\n",
    "def AnalysisBySudachi(sentences):\n",
    "  if sentences is np.nan:#欠損値は欠損値のまま返す\n",
    "    return np.NaN\n",
    "  text_list = sentences.split(\"。\")\n",
    "  results_list = []\n",
    "  for text in text_list:\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C #モードCの一番長い形で分ける\n",
    "    results =[m.surface() for m in tokenizer_obj.tokenize(text, mode)]\n",
    "    word_list = []\n",
    "    for word in results:\n",
    "      if (word != \"\") and (word !=\"、\"): #何故か分かち書きの結果として空白データ（''）ができたための省く処理\n",
    "        normalize = tokenizer_obj.tokenize(word, mode)[0].normalized_form() #正規化してなるべく言葉の揺れを無くす　e.g. 打込む → 打ち込む かつ丼 → カツ丼\n",
    "        pos = tokenizer_obj.tokenize(normalize, mode)[0].part_of_speech()[0]\n",
    "        if pos in  [\"名詞\", \"動詞\", \"形容詞\", \"副詞\"]:  # 対象とする品詞を指定\n",
    "          word = tokenizer_obj.tokenize(normalize, mode)[0].dictionary_form()\n",
    "          word_list.append(word)\n",
    "    results_list.append(\" \".join(word_list) + \"。\") #スペースで繋げていく\n",
    "  return \" \".join(results_list)\n",
    "\n",
    "#形態素解析結果を'sudachi'に代入\n",
    "data = pd.read_csv(storyPath+'cleaning.csv', index_col=0)\n",
    "data['summay_origin'] = data.cleaning.apply(AnalysisBySudachi)\n",
    "data.to_csv(storyPath+'summay_origin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "転職 Advent Calendar 2016 - Qiitaの14日目となります。 少しポエムも含みます。\n",
      "今年11月にSIerからWebサービスの会社へ転職しました。\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "corpus = [\n",
    "    '転職 Advent Calendar 2016 - Qiita 14 日 目 なる 少し ポエム 含む。',\n",
    "    '今年 11 月 SIer Web サービス 会社 転職 する。'\n",
    "]\n",
    "\n",
    "# 連結したcorpusを再度tinysegmenterでトークナイズさせる\n",
    "parser = PlaintextParser.from_string(''.join(corpus), Tokenizer('japanese'))\n",
    "\n",
    "# LexRankで要約を2文抽出\n",
    "summarizer = LexRankSummarizer()\n",
    "summarizer.stop_words = [' ']  # スペースも1単語として認識されるため、ストップワードにすることで除外する\n",
    "\n",
    "summary = summarizer(document=parser.document, sentences_count=2)\n",
    "\n",
    "# 元の文を表示\n",
    "for sentence in summary:\n",
    "    print(sentences[corpus.index(sentence.__str__())])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
