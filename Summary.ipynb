{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#local 読み込み\n",
    "dataPath = './datasets/'\n",
    "titlePath = './data/titledata/'\n",
    "keyPath = './data/keyworddata/'\n",
    "storyPath = './data/storydata/'\n",
    "allPath =  './data/all/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MecabでSummary用に形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "# MeCab による単語への分割関数,(名詞,形容詞,動詞)のみ残す\n",
    "def MorphologicalAnalysis(sentences):\n",
    "  if sentences is np.nan:#欠損値は欠損値のまま返す\n",
    "    return np.NaN\n",
    "  result = []\n",
    "  text_list = sentences.split(\"。\")\n",
    "  for text in text_list:\n",
    "    tagger = MeCab.Tagger()\n",
    "    words = []\n",
    "    for c in tagger.parse(text).splitlines()[:-1]:\n",
    "      surface, feature = c.split('\\t')\n",
    "      pos = feature.split(',')[0]\n",
    "      if pos in ['名詞', '動詞', '形容詞', '副詞']:\n",
    "        words.append(surface)\n",
    "    result.append(\" \".join(words)+\"。\")\n",
    "  return ' '.join(result)\n",
    "\n",
    "#形態素解析結果を'mecab'に代入\n",
    "data = pd.read_csv(storyPath+'cleaning.csv', index_col=0)\n",
    "data['summary_mecab'] = data['cleaning'].apply(MorphologicalAnalysis)\n",
    "data.to_csv(storyPath+'summary_mecab.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def makeSummary(corpus):\n",
    "    if corpus is np.nan:\n",
    "        return np.nan\n",
    "    # 連結したcorpusを再度tinysegmenterでトークナイズさせる\n",
    "    parser = PlaintextParser.from_string(''.join(corpus), Tokenizer('japanese'))\n",
    "    # LexRankで要約を3文抽出\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summarizer.stop_words = [' ']  # スペースも1単語として認識されるため、ストップワードにすることで除外する\n",
    "    summary = summarizer(document=parser.document, sentences_count=3)\n",
    "    return ''.join(map(str, summary))\n",
    "\n",
    "\n",
    "data = pd.read_csv(storyPath+'summary_mecab.csv')\n",
    "data['summary'] = data.summary_mecab.apply(makeSummary)\n",
    "data.to_csv(storyPath + 'summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sudachipy にて形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "\n",
    "# sudachi による単語への分割関数,(名詞,形容詞,動詞)のみ残す\n",
    "def AnalysisBySudachi(text):\n",
    "  if text is np.nan:#欠損値は欠損値のまま返す\n",
    "    return np.NaN\n",
    "  mode = tokenizer.Tokenizer.SplitMode.C #モードCの一番長い形で分ける\n",
    "  results =[m.surface() for m in tokenizer_obj.tokenize(text, mode)]\n",
    "  word_list = []\n",
    "  for word in results:\n",
    "    if not (word == \"\"): #何故か分かち書きの結果として空白データ（''）ができたための省く処理\n",
    "      normalize = tokenizer_obj.tokenize(word, mode)[0].normalized_form() #正規化（標準化？）してなるべく言葉の揺れを無くす　e.g. 打込む → 打ち込む かつ丼 → カツ丼\n",
    "      pos = tokenizer_obj.tokenize(normalize, mode)[0].part_of_speech()[0]\n",
    "      if pos in  [\"名詞\", \"動詞\", \"形容詞\"]:  # 対象とする品詞を指定\n",
    "        word = tokenizer_obj.tokenize(normalize, mode)[0].dictionary_form()\n",
    "        word_list.append(word)\n",
    "  return \" \".join(word_list) #スペースで繋げていく\n",
    "\n",
    "#形態素解析結果を'sudachi'に代入\n",
    "data = pd.read_csv(storyPath+'summary.csv', index_col=0)\n",
    "data['summary_sudachi'] = data['summary'].apply(AnalysisBySudachi)\n",
    "data.to_csv(storyPath+'summary_sudachi.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
