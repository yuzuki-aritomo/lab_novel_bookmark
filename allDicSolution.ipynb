{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#local 読み込み\n",
    "dataPath = './datasets/'\n",
    "titlePath = './data/titledata/'\n",
    "keyPath = './data/keyworddata/'\n",
    "storyPath = './data/storydata/'\n",
    "allPath =  './data/all/'\n",
    "\n",
    "test = pd.read_csv(dataPath + 'test.csv')\n",
    "train = pd.read_csv(dataPath +'train.csv')\n",
    "sub = pd.read_csv(dataPath +'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sudachi\"\n",
    "\n",
    "title_mecab_data = pd.read_csv(titlePath+'sudachi.csv')\n",
    "key_mecab_data = pd.read_csv(keyPath+'sudachi.csv' )\n",
    "story_mecab_data = pd.read_csv(storyPath+'sudachi.csv')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"sentence\"] = title_mecab_data[\"sudachi\"] + \" \" + key_mecab_data[\"sudachi\"] + \" \"+ story_mecab_data[\"sudachi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SaveLoad.save of <gensim.corpora.dictionary.Dictionary object at 0x000002BE283715E0>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "title_mecab_data = pd.read_csv(titlePath+'mecab.csv')\n",
    "key_mecab_data = pd.read_csv(keyPath+'cleaning.csv' )\n",
    "story_mecab_data = pd.read_csv(storyPath+'mecab.csv')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"sentence\"] = title_mecab_data[\"mecab\"] + \" \" + key_mecab_data[\"cleaning\"] + \" \"+ story_mecab_data[\"mecab\"]\n",
    "\n",
    "#[[文章の単語], [文章の単語]]を作成\n",
    "title_dic = []\n",
    "def make_title_dic(text):\n",
    "  if text is np.nan:\n",
    "    return\n",
    "  title_dic.append(text.split(' '))\n",
    "data['sentence'].map(make_title_dic)\n",
    "\n",
    "#学習\n",
    "model = word2vec.Word2Vec(title_dic,vector_size=50, min_count=5, window=5, epochs=20)\n",
    "\n",
    "#作成した辞書をcsvに保存して可視化(処理には関係ない)\n",
    "from gensim import corpora#辞書を作るためのもの\n",
    "dictionary = corpora.Dictionary(title_dic)#textsをもとに辞書を作成します\n",
    "dictionary.filter_extremes(no_below=5)#出現文書数が5回以下のものはさようなら\n",
    "dictionary.save_as_text(allPath + 'dictionary.csv')#辞書を保存\n",
    "dictionary.save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトルの平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 完了\n",
      "story 完了\n"
     ]
    }
   ],
   "source": [
    "dimension = 100 #次元\n",
    "\n",
    "#文章の分散表現を求める(単語ベクトルの平均)\n",
    "def getSentenceVector(text):\n",
    "  if text is np.nan:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  L = []\n",
    "  for w in text.split(' '):\n",
    "    if w in model.wv.key_to_index:\n",
    "      L.append(model.wv.get_vector(key=w))\n",
    "  if len(L)==0:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  return np.array(L).mean(axis=0)\n",
    "\n",
    "#分散表現をnp.arrayの形に変換\n",
    "def makeVectorArrayList(data, rowName):\n",
    "  vec_list = []\n",
    "  for index, row in data.iterrows():\n",
    "    vec_list.append(getSentenceVector(row[rowName]))\n",
    "  return np.array(vec_list)\n",
    "\n",
    "data = pd.read_csv(titlePath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'title_vec_originalLearned_sudachi_'+ str(dimension) +'.csv')\n",
    "\n",
    "print(\"title 完了\")\n",
    "\n",
    "data = pd.read_csv(storyPath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'story_vec_originalLearned_sudachi_'+ str(dimension) +'.csv')\n",
    "\n",
    "print(\"story 完了\")\n",
    "\n",
    "data = pd.read_csv(keyPath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'key_vec_originalLearned_sudachi_'+ str(dimension) +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "title_mecab_data = pd.read_csv(titlePath+'mecab.csv')\n",
    "key_mecab_data = pd.read_csv(keyPath+'cleaning.csv' )\n",
    "story_mecab_data = pd.read_csv(storyPath+'mecab.csv')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"sentence\"] = title_mecab_data[\"mecab\"] + \" \" + key_mecab_data[\"cleaning\"] + \" \"+ story_mecab_data[\"mecab\"]\n",
    "\n",
    "dimension = 50 #次元\n",
    "\n",
    "#単語辞書作成作成\n",
    "docs = []\n",
    "def make_docs(text):\n",
    "  if text is np.nan:\n",
    "    docs.append('')\n",
    "  else:\n",
    "    docs.append(text)\n",
    "data['sentence'].map(make_docs)\n",
    "# tf-idfの計算\n",
    "vectorizer = TfidfVectorizer(max_df=0.9) #文書全体の90%以上で出現する単語は無視する\n",
    "td_idf_list = vectorizer.fit_transform(docs)\n",
    "td_idf_vocabulary =  vectorizer.vocabulary_\n",
    "\n",
    "#文章の分散表現を求める(単語ベクトルの平均)\n",
    "def getSentenceVector(text, id):\n",
    "  if text is np.nan:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  L = []\n",
    "  for w in text.split(' '):\n",
    "    if w in model.wv.key_to_index:# modelにある時\n",
    "      vector = model.wv.get_vector(key=w)\n",
    "      if w in td_idf_vocabulary: # TD-IDFにある時\n",
    "        td_idf = td_idf_list[id, td_idf_vocabulary[w]] #td-idf を取得\n",
    "        L.append(list(map(lambda x: x*td_idf, vector))) # TD-IDFを掛ける\n",
    "  if len(L)==0:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  return np.array(L).mean(axis=0)\n",
    "\n",
    "#分散表現をnp.arrayの形に変換\n",
    "def makeVectorArrayList(data, rowName):\n",
    "  vec_list = []\n",
    "  for id, row in data.iterrows():\n",
    "    vec_list.append(getSentenceVector(row[rowName], id))\n",
    "  return np.array(vec_list)\n",
    "\n",
    "# data = pd.read_csv(titlePath+'mecab.csv', index_col=0)\n",
    "# vec_list_array = makeVectorArrayList(data, \"mecab\")\n",
    "# vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "# vec_data.to_csv(allPath+'title_mecab_vec_tdidf_'+ str(dimension) +'.csv')\n",
    "\n",
    "# print(\"title 完了\")\n",
    "\n",
    "# data = pd.read_csv(storyPath+'mecab.csv', index_col=0)\n",
    "# vec_list_array = makeVectorArrayList(data, \"mecab\")\n",
    "# vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "# vec_data.to_csv(allPath+'story_vec_mecab_tdidf_'+ str(dimension) +'.csv')\n",
    "\n",
    "# print(\"story 完了\")\n",
    "\n",
    "data = pd.read_csv(keyPath+'cleaning.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"cleaning\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'key_vec_mecab_tdidf_'+ str(dimension) +'.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f371c1edc13f10b1623f8473fa71877fe279074a94f999a2097c9dc15f19259"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
