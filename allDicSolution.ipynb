{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#local 読み込み\n",
    "dataPath = './datasets/'\n",
    "titlePath = './data/titledata/'\n",
    "keyPath = './data/keyworddata/'\n",
    "storyPath = './data/storydata/'\n",
    "allPath =  './data/all/'\n",
    "\n",
    "test = pd.read_csv(dataPath + 'test.csv')\n",
    "train = pd.read_csv(dataPath +'train.csv')\n",
    "sub = pd.read_csv(dataPath +'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sudachi\"\n",
    "\n",
    "title_mecab_data = pd.read_csv(titlePath+'sudachi.csv')\n",
    "key_mecab_data = pd.read_csv(keyPath+'sudachi.csv' )\n",
    "story_mecab_data = pd.read_csv(storyPath+'sudachi.csv')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"sentence\"] = title_mecab_data[\"sudachi\"] + \" \" + key_mecab_data[\"sudachi\"] + \" \"+ story_mecab_data[\"sudachi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SaveLoad.save of <gensim.corpora.dictionary.Dictionary object at 0x000001635A979040>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "#[[文章の単語], [文章の単語]]を作成\n",
    "title_dic = []\n",
    "def make_title_dic(text):\n",
    "  if text is np.nan:\n",
    "    return\n",
    "  title_dic.append(text.split(' '))\n",
    "data['sentence'].map(make_title_dic)\n",
    "\n",
    "#学習\n",
    "model = word2vec.Word2Vec(title_dic,vector_size=50, min_count=5, window=5, epochs=20)\n",
    "\n",
    "#作成した辞書をcsvに保存して可視化(処理には関係ない)\n",
    "from gensim import corpora#辞書を作るためのもの\n",
    "dictionary = corpora.Dictionary(title_dic)#textsをもとに辞書を作成します\n",
    "dictionary.filter_extremes(no_below=5)#出現文書数が5回以下のものはさようなら\n",
    "dictionary.save_as_text(allPath + 'dictionary.csv')#辞書を保存\n",
    "dictionary.save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトルの平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 完了\n",
      "story 完了\n"
     ]
    }
   ],
   "source": [
    "dimension = 100 #次元\n",
    "\n",
    "#文章の分散表現を求める(単語ベクトルの平均)\n",
    "def getSentenceVector(text):\n",
    "  if text is np.nan:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  L = []\n",
    "  for w in text.split(' '):\n",
    "    if w in model.wv.key_to_index:\n",
    "      L.append(model.wv.get_vector(key=w))\n",
    "  if len(L)==0:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  return np.array(L).mean(axis=0)\n",
    "\n",
    "#分散表現をnp.arrayの形に変換\n",
    "def makeVectorArrayList(data, rowName):\n",
    "  vec_list = []\n",
    "  for index, row in data.iterrows():\n",
    "    vec_list.append(getSentenceVector(row[rowName]))\n",
    "  return np.array(vec_list)\n",
    "\n",
    "data = pd.read_csv(titlePath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'title_vec_originalLearned_sudachi_'+ str(dimension) +'.csv')\n",
    "\n",
    "print(\"title 完了\")\n",
    "\n",
    "data = pd.read_csv(storyPath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'story_vec_originalLearned_sudachi_'+ str(dimension) +'.csv')\n",
    "\n",
    "print(\"story 完了\")\n",
    "\n",
    "data = pd.read_csv(keyPath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'key_vec_originalLearned_sudachi_'+ str(dimension) +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data[\"sentence\"] = title_mecab_data[\"sudachi\"] + \" \" + key_mecab_data[\"sudachi\"] + \" \"+ story_mecab_data[\"sudachi\"]\n",
    "\n",
    "dimension = 50 #次元\n",
    "\n",
    "#単語辞書作成作成\n",
    "docs = []\n",
    "def make_docs(text):\n",
    "  if text is np.nan:\n",
    "    docs.append('')\n",
    "  else:\n",
    "    docs.append(text)\n",
    "data['sentence'].map(make_docs)\n",
    "# tf-idfの計算\n",
    "vectorizer = TfidfVectorizer(max_df=0.9) #文書全体の90%以上で出現する単語は無視する\n",
    "td_idf_list = vectorizer.fit_transform(docs)\n",
    "td_idf_vocabulary =  vectorizer.vocabulary_\n",
    "\n",
    "#文章の分散表現を求める(単語ベクトルの平均)\n",
    "def getSentenceVector(text, id):\n",
    "  if text is np.nan:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  L = []\n",
    "  for w in text.split(' '):\n",
    "    if w in model.wv.key_to_index:# modelにある時\n",
    "      vector = model.wv.get_vector(key=w)\n",
    "      if w in td_idf_vocabulary: # TD-IDFにある時\n",
    "        td_idf = td_idf_list[id, td_idf_vocabulary[w]] #td-idf を取得\n",
    "        L.append(list(map(lambda x: x*td_idf, vector))) # TD-IDFを掛ける\n",
    "  if len(L)==0:\n",
    "    return np.array([np.nan for _ in range(dimension)])\n",
    "  return np.array(L).mean(axis=0)\n",
    "\n",
    "#分散表現をnp.arrayの形に変換\n",
    "def makeVectorArrayList(data, rowName):\n",
    "  vec_list = []\n",
    "  for id, row in data.iterrows():\n",
    "    vec_list.append(getSentenceVector(row[rowName], id))\n",
    "  return np.array(vec_list)\n",
    "\n",
    "data = pd.read_csv(titlePath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'title_vec_tdidf_'+ str(dimension) +'.csv')\n",
    "\n",
    "print(\"title 完了\")\n",
    "\n",
    "data = pd.read_csv(storyPath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'story_vec_tdidf_'+ str(dimension) +'.csv')\n",
    "\n",
    "print(\"story 完了\")\n",
    "\n",
    "data = pd.read_csv(keyPath+'sudachi.csv', index_col=0)\n",
    "vec_list_array = makeVectorArrayList(data, \"sudachi\")\n",
    "vec_data = pd.DataFrame(data=vec_list_array, dtype='float')\n",
    "vec_data.to_csv(allPath+'key_vec_tdidf_'+ str(dimension) +'.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 9 3 1\n",
      "['すべて', 'において', 'ドキュメント', '付けられる', '使える', '出現した', '情報検索', '総数', '重み付け', '集合']\n",
      "  (0, 3)\t0.4521233082633858\n",
      "  (0, 1)\t0.34385142961748544\n",
      "  (0, 9)\t0.4521233082633858\n",
      "  (0, 2)\t0.6877028592349709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngnls\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    'ドキュメント 集合 において ドキュメント の 単語 に 付けられる',\n",
    "    '情報検索 において 単語 へ の 重み付け に 使える',\n",
    "    'ドキュメント で 出現した すべて の 単語 の 総数',\n",
    "]\n",
    "vectorizer = TfidfVectorizer(max_df=0.9) # tf-idfの計算\n",
    "X = vectorizer.fit_transform(docs)\n",
    "w = vectorizer.vocabulary_\n",
    "print(w['ドキュメント'], w['集合'], w['付けられる'], w['において'])\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    暗い 窓辺 皇帝 北 宮 ファンタジー SF 天使 小人 猫 新しい 手直し 増し 宇宙 開...\n",
      "Name: sentence, dtype: object\n",
      "44527\n",
      "53572\n",
      "0.12312410733979824\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "row index (28738) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-98a1222aab54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_idf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtd_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd_idf_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m44527\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtd_idf_vocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'保育園'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#td-idf を取得\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \"\"\"\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Dispatch to specialized methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mM\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'row index (%d) out of range'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: row index (28738) out of range"
     ]
    }
   ],
   "source": [
    "data[\"sentence\"] = title_mecab_data[\"sudachi\"] + \" \" + key_mecab_data[\"sudachi\"] + \" \"+ story_mecab_data[\"sudachi\"]\n",
    "print(data[\"sentence\"][0:1])\n",
    "\n",
    "print(td_idf_vocabulary['暗い'])\n",
    "print(td_idf_vocabulary['窓辺'])\n",
    "# print(td_idf_list[44526])\n",
    "# print(td_idf_list[44527])\n",
    "print(td_idf_list[0, 44527])\n",
    "print(type(td_idf_list))\n",
    "\n",
    "td_idf = td_idf_list[44527][td_idf_vocabulary['保育園']] #td-idf を取得\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f371c1edc13f10b1623f8473fa71877fe279074a94f999a2097c9dc15f19259"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
